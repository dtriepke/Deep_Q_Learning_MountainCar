{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: DQNs\n",
    "\n",
    "source: https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "This project is about solving a reinforcement problem with an algorithm so called deep Q-Learning. \n",
    "\n",
    "This problem was first solved by the researchers from Google DeepMind. This tutorial is based on the main ideas from their early research papers (especially this and this). \n",
    "\n",
    "## Deep Q-Network\n",
    "DQN is introduced in 2 papers, Playing Atari with Deep Reinforcement Learning on NIPS in 2013 and Human-level control through deep reinforcement learning on Nature in 2015. Interestingly, there were only few papers about DRN between 2013 and 2015. I guess that the reason was people couldn’t reproduce DQN implementation without information in Nature version.\n",
    "\n",
    "More formally, this project uses artificial neural networks as non-linear function approximator with weights $\\theta$ for the action-value function:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}_{\\theta}(i_t) \\approx q^*(i_t, a_t) = \\mathbb{E} [ r_{t+1} + \\gamma \\max_{a} q^*(i_{t+1}, a_{t+1} ) ]\n",
    "$$\n",
    "where $i$ is a observed state and $a$ the choosen action. \n",
    "\n",
    "\n",
    "**Q Learning**\n",
    "  \n",
    "\n",
    "In the structure of the agent are two separate models placed. Each is a neural network with the same hyperparameter, but they differ temporarely in the parameter vector. \n",
    "\n",
    "$\\mathcal{N}_{\\theta_1}(i)$, $\\mathcal{N}_{\\theta_2}(i)$\n",
    "\n",
    "The first network, the action model, is responsible for the estimation of the action during the experimental phase. It outputs the q values on which the agent actual decide which action to take given a state.\n",
    "The second model, on the other side, has the task to provide the target q values for training the first network. \n",
    "Therefore it is called the target model and uses the replay memory during the learning phase.\n",
    "\n",
    "The idea behind the replay experience is:\n",
    "\n",
    "- For the basic Q-learning algorithm we need many thousand states from the game-environment in order to learn important features so the Q-values can be estimated.\n",
    "- Experience Replay is originally proposed in Reinforcement Learning for Robots Using Neural Networks in 1993. DNN is easily overfitting current episodes. Once DNN is overfitted, it’s hard to produce various experiences. To solve this problem, Experience Replay stores experiences including state transitions, rewards and actions, which are necessary data to perform Q learning, and makes mini-batches to update neural networks. This technique expects the following merits:\n",
    "\n",
    "    - reduces correlation between experiences in updating DNN\n",
    "    - increases learning speed with mini-batches\n",
    "    - reuses past transitions to avoid catastrophic forgetting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The learning or replay is invoked when the memory is full of recording from the experiment. The target model takes then each memory recording and does the q learning update: \n",
    "\n",
    "\n",
    "## Game Environment\n",
    "\n",
    "The playbox from `openAI` for developing and comparing reinforcement learning algorithms is the library called `gym`.\n",
    "This library inclued several environments or test problems that can be solved with reinforcement algorithm. \n",
    "It provides easy shared interfaces, which enables to skip the complex manual feature engineering. \n",
    "\n",
    "\n",
    "This project captures the learning problem `MountainCar`. \n",
    "Here is the challenge that a car, stocked between two hills, need to climb the right hill, but the a single impulse cause a to less momentum. The only way to solve the problem is that the agent drives front and back to generate a stronger momentum. \n",
    "Typically, the agent does not know about this approach and how to solve this efficiently.\n",
    "A Moore, Efficient Memory-Based Learning for Robot Control, PhD thesis, University of Cambridge, 1990. first discribed the problem.\n",
    "\n",
    "![](mountainCar.png)\n",
    "\n",
    "This is the `MountainCar` evironment from gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaces for the action is disrcet and there are 3 possible actions availible.\n",
    "$$\n",
    "a \\in \\mathcal{A} = \\{0, 1, 2\\}\n",
    "$$\n",
    "\n",
    "\n",
    "number | action  \n",
    "-------|-------  \n",
    "0      | push left\n",
    "1      | no operation\n",
    "2      | push right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space $\\boldsymbol{i}$ is an `2` dimensional vector. The first dimention tells the position of the car and the second the velocity, whereas both values are continious and fall into two intervalls:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{i} = (i_1, i_2)' \\in \\mathcal{S} = [-1.2, 0.6] \\times [-0.07, 0.07]\n",
    "$$\n",
    "\n",
    "number | action  \n",
    "-------|-------  \n",
    "$i_1$  | position\n",
    "$i_2$  | velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "Lower bound is :: [-1.2  -0.07]\n",
      "Upper bound is :: [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(\"Lower bound is :: {}\".format(env.observation_space.low))\n",
    "print(\"Upper bound is :: {}\".format(env.observation_space.high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward \n",
    "\n",
    "The reward is set to be -1 for each time step except the goal position of $0.5$ is reached.\n",
    "$$\n",
    "r \\in \\mathcal{R} = \\{-1, 0\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal State\n",
    "The terminal state determnines the end of an epsiode and is either when the car is in state $\\boldsymbol{i}_{200}$ or in the state $\\boldsymbol{i}_t = (0.5, i_{t2})$ with $t \\leq 200$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import deepQLearningSimple as dql\n",
    "import gym\n",
    "from keras.models import load_model \n",
    "from gym.envs.classic_control.mountain_car import MountainCarEnv\n",
    "from gym.wrappers.time_limit import TimeLimit\n",
    "\n",
    "def PatientMountainCar():\n",
    "    env = MountainCarEnv()\n",
    "    return TimeLimit(env, max_episode_steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      " Game :: 1 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: 0.05103093758225441  \n",
      "Replay memory is sufficient full, start with inner trainings loop.\n",
      "clear\n",
      " Game :: 2 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: 0.04776307940483093  \n",
      " Game :: 3 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: 0.05203607678413391  \n",
      "clear\n",
      " Game :: 4 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: 0.052099358290433884  \n",
      " Game :: 5 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: -0.020885754376649857  \n",
      "clear\n",
      " Game :: 6 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: -0.020759379491209984  \n",
      " Game :: 7 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: 0.0991588905453682  \n",
      "clear\n",
      " Game :: 8 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: 0.0989929586648941  \n",
      " Game :: 9 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: -0.0026796322781592607  \n",
      "clear\n",
      " Game :: 10 Wins :: 0 Steps :: 500 Reward -500.0 Mean Q Value :: -0.0026837654877454042  \n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#env = gym.make(\"MountainCar-v0\")\n",
    "env = PatientMountainCar()\n",
    "agent = dql.agent(env  = env, training = True, render = False)\n",
    "\n",
    "# Training\n",
    "agent.run(num_episode = 10, num_steps = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_changing_rate_layer(model, layer): \n",
    "    smoothing_factor = 0.00001\n",
    "    \n",
    "    # Get the layer for all experiments from the history\n",
    "    idx = range(len(model.weights_history))\n",
    "    l = [model.weights_history[i][layer]  for i in idx]\n",
    "    \n",
    "    # Calculate the mean deviation from the privious weights\n",
    "    idx = range(len(l))\n",
    "    changing_rate = [np.mean( (l[i + 1] + smoothing_factor) / \n",
    "                                 (l[i] + smoothing_factor)) for i in idx if i < len(l) -1]\n",
    "\n",
    "    return changing_rate\n",
    "\n",
    "changing_l1 = mean_changing_rate_layer(agent.action_dqn, 0)\n",
    "changing_l2 = mean_changing_rate_layer(agent.action_dqn, 1)\n",
    "changing_l3 = mean_changing_rate_layer(agent.action_dqn, 2)\n",
    "changing_l4 = mean_changing_rate_layer(agent.action_dqn, 3)\n",
    "\n",
    "changing_l8 = mean_changing_rate_layer(agent.action_dqn, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(changing_l1, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(changing_l3, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agentDQN.replay_memory.memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run testing \n",
    "agentDQN.training = False\n",
    "agentDQN.render = True\n",
    "\n",
    "agentDQN.run(num_episode = 10, num_steps = 500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
