{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: DQNs\n",
    "\n",
    "source: https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "This project is about solving a reinforcement problem with an algorithm so called deep Q-Learning. \n",
    "\n",
    "This problem was first solved by the researchers from Google DeepMind. This tutorial is based on the main ideas from their early research papers (especially this and this). \n",
    "\n",
    "## Deep Q-Network\n",
    "DQN is introduced in 2 papers, Playing Atari with Deep Reinforcement Learning on NIPS in 2013 and Human-level control through deep reinforcement learning on Nature in 2015. Interestingly, there were only few papers about DRN between 2013 and 2015. I guess that the reason was people couldn’t reproduce DQN implementation without information in Nature version.\n",
    "\n",
    "More formally, this project uses artificial neural networks as non-linear function approximator with weights $\\theta$ for the action-value function:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}_{\\theta}(i_t) \\approx q^*(i_t, a_t) = \\mathbb{E} [ r_{t+1} + \\gamma \\max_{a} q^*(i_{t+1}, a_{t+1} ) ]\n",
    "$$\n",
    "where $i$ is a observed state and $a$ the choosen action. \n",
    "\n",
    "\n",
    "**Q Learning**\n",
    "  \n",
    "\n",
    "In the structure of the agent are two separate models placed. Each is a neural network with the same hyperparameter, but they differ temporarely in the parameter vector. \n",
    "\n",
    "$\\mathcal{N}_{\\theta_1}(i)$, $\\mathcal{N}_{\\theta_2}(i)$\n",
    "\n",
    "The first network, the action model, is responsible for the estimation of the action during the experimental phase. It outputs the q values on which the agent actual decide which action to take given a state.\n",
    "The second model, on the other side, has the task to provide the target q values for training the first network. \n",
    "Therefore it is called the target model and uses the replay memory during the learning phase.\n",
    "\n",
    "The idea behind the replay experience is:\n",
    "\n",
    "- For the basic Q-learning algorithm we need many thousand states from the game-environment in order to learn important features so the Q-values can be estimated.\n",
    "- Experience Replay is originally proposed in Reinforcement Learning for Robots Using Neural Networks in 1993. DNN is easily overfitting current episodes. Once DNN is overfitted, it’s hard to produce various experiences. To solve this problem, Experience Replay stores experiences including state transitions, rewards and actions, which are necessary data to perform Q learning, and makes mini-batches to update neural networks. This technique expects the following merits:\n",
    "\n",
    "    - reduces correlation between experiences in updating DNN\n",
    "    - increases learning speed with mini-batches\n",
    "    - reuses past transitions to avoid catastrophic forgetting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The learning or replay is invoked when the memory is full of recording from the experiment. The target model takes then each memory recording and does the q learning update: \n",
    "\n",
    "\n",
    "## Game Environment\n",
    "\n",
    "The playbox from `openAI` for developing and comparing reinforcement learning algorithms is the library called `gym`.\n",
    "This library inclued several environments or test problems that can be solved with reinforcement algorithm. \n",
    "It provides easy shared interfaces, which enables to skip the complex manual feature engineering. \n",
    "\n",
    "\n",
    "This project captures the learning problem `MountainCar`. \n",
    "Here is the challenge that a car, stocked between two hills, need to climb the right hill, but the a single impulse cause a to less momentum. The only way to solve the problem is that the agent drives front and back to generate a stronger momentum. \n",
    "Typically, the agent does not know about this approach and how to solve this efficiently.\n",
    "A Moore, Efficient Memory-Based Learning for Robot Control, PhD thesis, University of Cambridge, 1990. first discribed the problem.\n",
    "\n",
    "![](mountainCar.png)\n",
    "\n",
    "This is the `MountainCar` evironment from gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaces for the action is disrcet and there are 3 possible actions availible.\n",
    "$$\n",
    "a \\in \\mathcal{A} = \\{0, 1, 2\\}\n",
    "$$\n",
    "\n",
    "\n",
    "number | action  \n",
    "-------|-------  \n",
    "0      | push left\n",
    "1      | no operation\n",
    "2      | push right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space $\\boldsymbol{i}$ is an `2` dimensional vector. The first dimention tells the position of the car and the second the velocity, whereas both values are continious and fall into two intervalls:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{i} = (i_1, i_2)' \\in \\mathcal{S} = [-1.2, 0.6] \\times [-0.07, 0.07]\n",
    "$$\n",
    "\n",
    "number | action  \n",
    "-------|-------  \n",
    "$i_1$  | position\n",
    "$i_2$  | velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "Lower bound is :: [-1.2  -0.07]\n",
      "Upper bound is :: [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(\"Lower bound is :: {}\".format(env.observation_space.low))\n",
    "print(\"Upper bound is :: {}\".format(env.observation_space.high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward \n",
    "\n",
    "The reward is set to be -1 for each time step except the goal position of $0.5$ is reached.\n",
    "$$\n",
    "r \\in \\mathcal{R} = \\{-1, 0\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal State\n",
    "The terminal state determnines the end of an epsiode and is either when the car is in state $\\boldsymbol{i}_{200}$ or in the state $\\boldsymbol{i}_t = (0.5, i_{t2})$ with $t \\leq 200$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Traing\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import deepQLearningSimple as dql\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      " Game :: 1 Wins :: 0 Mean Q Value :: 0.024898752570152283  Median Q Values :: 0.025068996474146843 \n",
      " Game :: 2 Wins :: 0 Mean Q Value :: 0.025488073006272316  Median Q Values :: 0.02617967128753662 \n",
      " Game :: 3 Wins :: 0 Mean Q Value :: 0.025484977290034294  Median Q Values :: 0.026033785194158554 \n",
      " Game :: 4 Wins :: 0 Mean Q Value :: 0.02517753280699253  Median Q Values :: 0.02536056563258171 \n",
      " Game :: 5 Wins :: 0 Mean Q Value :: 0.025322481989860535  Median Q Values :: 0.025494081899523735 \n",
      " Game :: 6 Wins :: 0 Mean Q Value :: 0.024952402338385582  Median Q Values :: 0.024924002587795258 \n",
      " Game :: 7 Wins :: 0 Mean Q Value :: 0.024831285700201988  Median Q Values :: 0.024779269471764565 \n",
      " Game :: 8 Wins :: 0 Mean Q Value :: 0.02498335763812065  Median Q Values :: 0.025037799030542374 \n",
      " Game :: 9 Wins :: 0 Mean Q Value :: 0.02494696155190468  Median Q Values :: 0.024913951754570007 \n",
      "Replay memory is sufficient full, start with inner trainings loop.\n",
      " Game :: 10 Wins :: 0 Mean Q Value :: 0.02503499574959278  Median Q Values :: 0.02507743053138256 \n",
      " Game :: 11 Wins :: 0 Mean Q Value :: -0.28672513365745544  Median Q Values :: -0.2502027153968811 \n",
      " Game :: 12 Wins :: 0 Mean Q Value :: -1.5301414728164673  Median Q Values :: -1.46412992477417 \n",
      " Game :: 13 Wins :: 0 Mean Q Value :: -3.7693450450897217  Median Q Values :: -3.743788719177246 \n",
      " Game :: 14 Wins :: 0 Mean Q Value :: -6.888476371765137  Median Q Values :: -6.805432319641113 \n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "agent = dql.agent(env  = env, training = True, render = False)\n",
    "\n",
    "\n",
    "# Training\n",
    "agent.run(num_episode = 10000, num_steps = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_changing_rate_layer(model, layer): \n",
    "    smoothing_factor = 0.00001\n",
    "    \n",
    "    # Get the layer for all experiments from the history\n",
    "    idx = range(len(model.weights_history))\n",
    "    l = [model.weights_history[i][layer]  for i in idx]\n",
    "    \n",
    "    # Calculate the mean deviation from the privious weights\n",
    "    idx = range(len(l))\n",
    "    changing_rate = [np.mean( (l[i + 1] + smoothing_factor) / \n",
    "                                 (l[i] + smoothing_factor)) for i in idx if i < len(l) -1]\n",
    "\n",
    "    return changing_rate\n",
    "\n",
    "changing_l1 = mean_changing_rate_layer(agent.action_dqn, 0)\n",
    "changing_l2 = mean_changing_rate_layer(agent.action_dqn, 1)\n",
    "changing_l3 = mean_changing_rate_layer(agent.action_dqn, 2)\n",
    "changing_l4 = mean_changing_rate_layer(agent.action_dqn, 3)\n",
    "\n",
    "changing_l8 = mean_changing_rate_layer(agent.action_dqn, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(changing_l1, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(changing_l3, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agentDQN.replay_memory.memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run testing \n",
    "agentDQN.training = False\n",
    "agentDQN.render = True\n",
    "\n",
    "agentDQN.run(num_episode = 10, num_steps = 500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
